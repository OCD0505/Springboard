{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9cf8df5e",
   "metadata": {},
   "source": [
    "# Springboard Data Science Track Notes\n",
    "--------------------------------------\n",
    "### Program Start Date: 07/10/2023 - Expected End Date 12/10/2023\n",
    "--------------------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "158c1499",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "918365f3",
   "metadata": {},
   "source": [
    "#### DataCamp Courses completed so far\n",
    "- Introduction to Python\n",
    "- Data Types for Data Science in Python\n",
    "- Intermediate Python\n",
    "- Python Data Science Toolbox (Part 1)\n",
    "- Joining Data with Pandas\n",
    "- Data Manipulation with Pandas\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc80de99",
   "metadata": {},
   "source": [
    "07/03/2023\n",
    "\n",
    "- Explored setting the path for anaconda3, using HomeBrew to install via the terminal, connecting Github to Jupyter/Local Drive\n",
    "- Intro to Data Science\n",
    "    - OSEMN, REPL, and ECRD\n",
    "    - OSEMN: Obtain data, Scrub data, Explore data, Model data, iNterpret data\n",
    "    - REPL: Read, Evaluate, Print, Loop <- Command Line\n",
    "    - ECRD: Edit, Compile, Run, Debug <- Scripts & Large Programs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93347954",
   "metadata": {},
   "source": [
    "07/05/2023\n",
    "\n",
    "- Explored Docker containers but put it on the backburner (not needed just yet) \n",
    "- Explored mapping local dir -> datasci@commanndline to Docker container -> /home/dst "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5de9011e",
   "metadata": {},
   "source": [
    "07/06/2023\n",
    "\n",
    "- Explored command line via terminal on MacOS, zsh\n",
    "- Continued DataCamp courses (see above for list of completed courses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27477aff",
   "metadata": {},
   "source": [
    "07/07/2023 - 07/10/2023 Statistics & Probability Review\n",
    "\n",
    "- Measures of central tendency (ie Mean, Median, Mode)\n",
    "    - Left Skewed vs Right Skewed vs Normal \n",
    "- Variance & Standard Deviation\n",
    "- Completed Intro to Shell on DataCamp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fdd2369",
   "metadata": {},
   "source": [
    "#### CRISP-DM | CRoss Industry Standard Process for Data Mining \n",
    "Industry Approach\n",
    "Link: https://www.datascience-pm.com/crisp-dm-2/\n",
    "- Outline\n",
    "    - Business understanding – What does the business need?\n",
    "    - Data understanding – What data do we have / need? Is it clean?\n",
    "    - Data preparation – How do we organize the data for modeling?\n",
    "    - Modeling – What modeling techniques should we apply?\n",
    "    - Evaluation – Which model best meets the business objectives?\n",
    "    - Deployment – How do stakeholders access the results?\n",
    "\n",
    "#### Springboard Approach\n",
    "Academic Approach\n",
    "Link: https://www.springboard.com/blog/data-science/data-science-process/\n",
    "- Outline\n",
    "    - Problem Id'ing\n",
    "    - Data Wrangling\n",
    "    - Exploratory Data Analysis\n",
    "    - Pre-processing & Training Data\n",
    "    - Modelling\n",
    "    - Documentation \n",
    "  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca64aaac",
   "metadata": {},
   "source": [
    "07/11/2023\n",
    "- Started Monalco Mining Case Study \n",
    "- See /Users/frankyaraujo/Desktop/Springboard/\"2023 July 11 _ Monalco Problem Statement _ Franky Araujo _.key\"\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e76e3435",
   "metadata": {},
   "source": [
    "07/12/2023 - Took a break "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f99e46ca",
   "metadata": {},
   "source": [
    "07/13/2023\n",
    "- Finished & Submitted Monalco Mining Case Study\n",
    "- Continued Springboard curriculum "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f19e522d",
   "metadata": {},
   "source": [
    "07/14/2023\n",
    "- Continued Springboard curriculum\n",
    "- Met with career coach, Mike Hernandez"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ffd7bd2",
   "metadata": {},
   "source": [
    "07/15/2023 - 07/20/2023 \n",
    "- Python courses on Datacamp\n",
    "    - Data Types for Data Science in Python\n",
    "- General Python practice & review\n",
    "- Completed Career Strategy Plan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c040a30a",
   "metadata": {},
   "source": [
    "07/21/2023 - Took a break to host at home"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50492561",
   "metadata": {},
   "source": [
    "07/22/2023 - 7/27/2023 \n",
    "- Intro to Kaggle\n",
    "- Python courses, working with Pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b89341f4",
   "metadata": {},
   "source": [
    "07/27/2023\n",
    "- Completed Data Manipulation with Pandas\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "b1f921d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   EmployeeID EmployeeName  ManagerID    HireDate  Salary\n",
      "0           1        Alice          3  2018-02-05   60000\n",
      "1           2          Bob          3  2017-04-01   55000\n",
      "2           3      Charlie          4  2012-06-15   75000\n",
      "3           4        David          4  2010-06-12   80000\n",
      "4           5          Eve          5  2016-04-23   65000\n",
      "5           6        Frank          6  2019-09-10   70000\n",
      "6           7        Grace          6  2020-03-20   72000\n",
      "7           8        Helen          7  2015-08-08   68000\n",
      "8           9          Ivy          7  2018-11-16   63000\n",
      "9          10         Jack          8  2021-02-28   59000\n",
      "   EmployeeID EmployeeName  ManagerID    HireDate  Salary\n",
      "0           1        Alice          3  2018-02-05   60000\n",
      "1           2          Bob          3  2017-04-01   55000\n",
      "2           3      Charlie          4  2012-06-15   75000\n",
      "3           4        David          4  2010-06-12   80000\n",
      "4           5          Eve          5  2016-04-23   65000\n",
      "       EmployeeID  ManagerID        Salary\n",
      "count    10.00000  10.000000     10.000000\n",
      "mean      5.50000   5.300000  66700.000000\n",
      "std       3.02765   1.766981   7775.317071\n",
      "min       1.00000   3.000000  55000.000000\n",
      "25%       3.25000   4.000000  60750.000000\n",
      "50%       5.50000   5.500000  66500.000000\n",
      "75%       7.75000   6.750000  71500.000000\n",
      "max      10.00000   8.000000  80000.000000\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10 entries, 0 to 9\n",
      "Data columns (total 5 columns):\n",
      " #   Column        Non-Null Count  Dtype \n",
      "---  ------        --------------  ----- \n",
      " 0   EmployeeID    10 non-null     int64 \n",
      " 1   EmployeeName  10 non-null     object\n",
      " 2   ManagerID     10 non-null     int64 \n",
      " 3   HireDate      10 non-null     object\n",
      " 4   Salary        10 non-null     int64 \n",
      "dtypes: int64(3), object(2)\n",
      "memory usage: 528.0+ bytes\n",
      "None\n",
      "[[1 'Alice' 3 '2018-02-05' 60000]\n",
      " [2 'Bob' 3 '2017-04-01' 55000]\n",
      " [3 'Charlie' 4 '2012-06-15' 75000]\n",
      " [4 'David' 4 '2010-06-12' 80000]\n",
      " [5 'Eve' 5 '2016-04-23' 65000]\n",
      " [6 'Frank' 6 '2019-09-10' 70000]\n",
      " [7 'Grace' 6 '2020-03-20' 72000]\n",
      " [8 'Helen' 7 '2015-08-08' 68000]\n",
      " [9 'Ivy' 7 '2018-11-16' 63000]\n",
      " [10 'Jack' 8 '2021-02-28' 59000]] \n",
      " Index(['EmployeeID', 'EmployeeName', 'ManagerID', 'HireDate', 'Salary'], dtype='object') \n",
      " RangeIndex(start=0, stop=10, step=1)\n",
      "0    60000\n",
      "1    60000\n",
      "2    75000\n",
      "3    80000\n",
      "4    80000\n",
      "5    80000\n",
      "6    80000\n",
      "7    80000\n",
      "8    80000\n",
      "9    80000\n",
      "Name: Salary, dtype: int64\n",
      "0    60000\n",
      "1    55000\n",
      "2    55000\n",
      "3    55000\n",
      "4    55000\n",
      "5    55000\n",
      "6    55000\n",
      "7    55000\n",
      "8    55000\n",
      "9    55000\n",
      "Name: Salary, dtype: int64\n",
      "0     60000\n",
      "1    115000\n",
      "2    190000\n",
      "3    270000\n",
      "4    335000\n",
      "5    405000\n",
      "6    477000\n",
      "7    545000\n",
      "8    608000\n",
      "9    667000\n",
      "Name: Salary, dtype: int64\n",
      "0                  60000\n",
      "1             3300000000\n",
      "2        247500000000000\n",
      "3    1353255926290448384\n",
      "4    7559465432002854912\n",
      "5    -720258232353816576\n",
      "6   -4795138277243879424\n",
      "7   -4754605693766467584\n",
      "8   -1928438391758651392\n",
      "9    1652332880082239488\n",
      "Name: Salary, dtype: int64\n",
      "     Salary    Bonus\n",
      "sum  667000  40020.0\n",
      "max   80000   4800.0\n",
      "min   55000   3300.0\n",
      "              Salary   Bonus\n",
      "EmployeeName                \n",
      "Alice          60000  3600.0\n",
      "Bob            55000  3300.0\n",
      "Charlie        75000  4500.0\n",
      "David          80000  4800.0\n",
      "Eve            65000  3900.0\n",
      "Frank          70000  4200.0\n",
      "Grace          72000  4320.0\n",
      "Helen          68000  4080.0\n",
      "Ivy            63000  3780.0\n",
      "Jack           59000  3540.0\n",
      "EmployeeID       1        2        3        4        5        6        7   \\\n",
      "HireDate                                                                    \n",
      "2010-06-12        -        -        -  80000.0        -        -        -   \n",
      "2012-06-15        -        -  75000.0        -        -        -        -   \n",
      "2015-08-08        -        -        -        -        -        -        -   \n",
      "2016-04-23        -        -        -        -  65000.0        -        -   \n",
      "2017-04-01        -  55000.0        -        -        -        -        -   \n",
      "2018-02-05  60000.0        -        -        -        -        -        -   \n",
      "2018-11-16        -        -        -        -        -        -        -   \n",
      "2019-09-10        -        -        -        -        -  70000.0        -   \n",
      "2020-03-20        -        -        -        -        -        -  72000.0   \n",
      "2021-02-28        -        -        -        -        -        -        -   \n",
      "\n",
      "EmployeeID       8        9        10  \n",
      "HireDate                               \n",
      "2010-06-12        -        -        -  \n",
      "2012-06-15        -        -        -  \n",
      "2015-08-08  68000.0        -        -  \n",
      "2016-04-23        -        -        -  \n",
      "2017-04-01        -        -        -  \n",
      "2018-02-05        -        -        -  \n",
      "2018-11-16        -  63000.0        -  \n",
      "2019-09-10        -        -        -  \n",
      "2020-03-20        -        -        -  \n",
      "2021-02-28        -        -  59000.0  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "#Example DF to show how the following code works\n",
    "data = {\n",
    "    'EmployeeID': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
    "    'EmployeeName': ['Alice', 'Bob', 'Charlie', 'David', 'Eve', 'Frank', 'Grace', 'Helen', 'Ivy', 'Jack'],\n",
    "    'ManagerID': [3, 3, 4, 4, 5, 6, 6, 7, 7, 8],\n",
    "    'HireDate': ['2018-02-05', '2017-04-01', '2012-06-15', '2010-06-12', '2016-04-23', '2019-09-10', '2020-03-20', '2015-08-08', '2018-11-16', '2021-02-28'],\n",
    "    'Salary': [60000, 55000, 75000, 80000, 65000, 70000, 72000, 68000, 63000, 59000]\n",
    "}\n",
    "\n",
    "\n",
    "df=pd.DataFrame(data)\n",
    "\n",
    "#Displaying Data Frame\n",
    "\n",
    "print(df)\n",
    "print(df.head()) #returns the first few rows of the dataset\n",
    "print(df.describe()) #The describe method computes some summary statistics for numerical columns,\n",
    "print(df.info()) #The info method displays the names of columns, the data types they contain, and whether they have any missing values\n",
    "print(df.values,\"\\n\", df.columns,\"\\n\", df.index)\n",
    "\n",
    "#Aggregating DataFrames\n",
    "\n",
    "print(df['Salary'].cummax())\n",
    "print(df['Salary'].cummin())\n",
    "print(df['Salary'].cumsum())\n",
    "print(df['Salary'].cumprod())\n",
    "\n",
    "df['Bonus']=df['Salary']*.06\n",
    "\n",
    "print(df[['Salary','Bonus']].agg([sum,max, min])) \n",
    "# The .agg() method allows you to apply your own custom functions to a DataFrame, \n",
    "# as well as apply functions to more than one column of a DataFrame at once, \n",
    "# making your aggregations super-efficient. For example, df['column'].agg(function)\n",
    "\n",
    "df['EmployeeName'].value_counts()\n",
    "#Note: Before counting values, make sure to clear out\n",
    "#duplicates using df.drop_duplicates(subset=\"column\")\n",
    "\n",
    "print(df.groupby('EmployeeName')[[\"Salary\",\"Bonus\"]].agg(sum)) # Example of groupby\n",
    "\n",
    "print(df.pivot_table(values='Salary',index='HireDate',columns='EmployeeID',sort=True, fill_value='-'))\n",
    "# Example of .pivot_table\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "138e2189",
   "metadata": {},
   "source": [
    "07/28/2023 - 07/29/2023\n",
    "- Started Joining Data with pandas\n",
    "    - df.merge\n",
    "        - Left Join, brings data from df2 to df1 and keeps all data in df1, df1.merge(df2, on=\"column\", how=\"left\" ...)\n",
    "        - Right Join, similar to above except how =\"right\"and all df2 data stays while data from df1 is added based on -> on = \"column\"\n",
    "        - Inner Join,set how=\"inner\",this only keeps select data from df1 and df2 based on -> on=\"column\"\n",
    "        - Outer Join, set how=\"outer\", this takes all data from df1 and df2 but fills missing values with NaN (Not a Number) \n",
    "        - Self Join, combine data to itself when there is related data within the dame dataset. For example, combining related rows into a single column:    \n",
    "<code>\n",
    "        <code>import pandas as pd\n",
    "\n",
    "      <code>  #Sample DataFrame\n",
    "      <code>  data = {\n",
    "      <code>      'EmployeeID': [1, 2, 3, 4, 5],\n",
    "      <code>      'EmployeeName': ['Alice', 'Bob', 'Charlie', 'David', 'Eve'],\n",
    "      <code>      'ManagerID': [3, 3, 4, 4, 5]\n",
    "      <code>  }\n",
    "          \n",
    "      <code>  df = pd.DataFrame(data)\n",
    "      <code>  print(df)\n",
    "\n",
    "        **Output:**\n",
    "       <code>         EmployeeID EmployeeName  ManagerID\n",
    "      <code>  0           1       Alice          3\n",
    "      <code>  1           2         Bob          3\n",
    "       <code> 2           3     Charlie          4\n",
    "      <code>  3           4       David          4\n",
    "       <code> 4           5         Eve          5\n",
    "\n",
    "\n",
    "       <code> #Self-Join\n",
    "       <code> result_self_join = df.merge(df, left_on='ManagerID', right_on='EmployeeID', suffixes=('_Employee', '_Manager'))\n",
    "\n",
    "      <code>  #Selecting relevant columns\n",
    "      <code>  result_self_join = result_self_join[['EmployeeName_Employee', 'EmployeeName_Manager']]\n",
    "\n",
    "      <code> print(result_self_join)\n",
    "\n",
    "        **Output:**\n",
    "       <code>        EmployeeName_Employee EmployeeName_Manager\n",
    "      <code>  0                 Alice             Charlie\n",
    "      <code>  1                   Bob             Charlie\n",
    "      <code>  2               Charlie               David\n",
    "      <code>  3                 David               David\n",
    "      <code>  4                   Eve                 Eve\n",
    "        <code>\n",
    "        - Semi-join effectively filters out rows in df1 based on their existence in df2, providing a subset of the original DataFrame that meets the matching condition.\n",
    "        <code>\n",
    "        import pandas as pd\n",
    "\n",
    "       <code> #Sample DataFrame for employees\n",
    "       <code> data1 = {\n",
    "       <code>     'EmployeeID': [1, 2, 3, 4, 5],\n",
    "       <code>     'EmployeeName': ['Alice', 'Bob', 'Charlie', 'David', 'Eve']\n",
    "       <code> }\n",
    "       <code> df1 = pd.DataFrame(data1)\n",
    "       <code> #Sample DataFrame with additional information for specific employees\n",
    "       <code> data2 = {\n",
    "           \n",
    "       <code>     'EmployeeID': [2, 4, 5],\n",
    "       <code>     'Age': [25, 30, 22]\n",
    "           \n",
    "       <code> }\n",
    "\n",
    "      <code> df2 = pd.DataFrame(data2)\n",
    "        \n",
    "      <code>      EmployeeID EmployeeName\n",
    "      <code>  0           1       Alice\n",
    "      <code>  1           2         Bob\n",
    "      <code>  2           3     Charlie\n",
    "      <code>  3           4       David\n",
    "      <code>  4           5         Eve\n",
    "\n",
    "      <code>     EmployeeID  Age\n",
    "      <code>  0           2   25\n",
    "      <code>  1           4   30\n",
    "      <code>  2           5   22\n",
    "\n",
    "\n",
    "      <code>  #Semi-Join\n",
    "      <code>  result_semi_join = df1.merge(df2, on='EmployeeID', how='inner')\n",
    "\n",
    "      <code>  print(result_semi_join)\n",
    "\n",
    "      <code>    EmployeeID EmployeeName  Age\n",
    "      <code>   0           2         Bob   25\n",
    "      <code>   1           4       David   30\n",
    "      <code>  2           5         Eve   22\n",
    "      <code>  \n",
    "        - Anti-join is a type of join operation that returns only the rows from the left DataFrame (or the original DataFrame) that do not have matching rows in the right DataFrame based on a common column or set of columns.\n",
    "            - Here's how the anti-join process works step-by-step:\n",
    "\n",
    "                1. Perform the Merge:\n",
    "\n",
    "                In pandas, you use the .merge() method (or pd.merge() function) to combine two DataFrames based on one or more common columns.\n",
    "                When performing an anti-join, you typically use a left join (how='left') to keep all rows from the left DataFrame and only the matching rows from the right DataFrame. This includes rows with matching values in the merging column(s) from both DataFrames.\n",
    "                \n",
    "                2. Indicate Match Status:\n",
    "\n",
    "                After the merge, a new column (by default named _merge) is added to the resulting DataFrame that indicates the match status for each row.\n",
    "                The _merge column takes three possible values: 'both' if the row has a match in both DataFrames, 'left_only' if the row has a match only in the left DataFrame, and 'right_only' if the row has a match only in the right DataFrame.\n",
    "                \n",
    "                3. Filter Rows:\n",
    "\n",
    "                To obtain the anti-join result, you filter out the rows where _merge is 'both', meaning they have matching values in both DataFrames. The resulting DataFrame will contain only the rows that have no corresponding matches.<code>\n",
    "                import pandas as pd\n",
    "    <code>\n",
    "                #Sample DataFrames\n",
    "                df1 = pd.DataFrame({'KeyColumn': [1, 2, 3, 4, 5], 'ColumnA': ['A', 'B', 'C', 'D', 'E']})\n",
    "                df2 = pd.DataFrame({'KeyColumn': [2, 4], 'ColumnB': ['X', 'Y']})\n",
    "    <code>\n",
    "                #Anti-Join\n",
    "                result_anti_join = df1.merge(df2, on='KeyColumn', how='left', indicator=True)\n",
    "                result_anti_join = result_anti_join[result_anti_join['_merge'] == 'left_only']\n",
    "                result_anti_join.drop(['ColumnB', '_merge'], axis=1, inplace=True)\n",
    "    <code>\n",
    "                print(result_anti_join)\n",
    ">\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "658dc611",
   "metadata": {},
   "source": [
    "7/29/2023 Continued...\n",
    "#### Examples of how .query, .concat, .merge_ordered, .merge_asof, and .melt work in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "cfd625fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example Data Set #1\n",
      "     Name  Age           City\n",
      "0   John   25       New York\n",
      "1   Jane   30  San Francisco\n",
      "2   Mike   22        Chicago\n",
      "3  Alice   28    Los Angeles\n",
      "4    Bob   35        Seattle \n",
      "\n",
      "Example Data Set #2\n",
      "      Name  Age         City     Sex\n",
      "0   Emily   28       London  Female\n",
      "1    Jack   24     New York    Male\n",
      "2    Emma   27        Paris  Female\n",
      "3   James   26  Los Angeles    Male\n",
      "4  Olivia   29       Sydney  Female \n",
      "\n",
      "After df.query applied to Data Set #1\n",
      "\n",
      "     Name  Age         City\n",
      "3  Alice   28  Los Angeles\n",
      "4    Bob   35      Seattle\n",
      "\n",
      "After df.concat applied to Data Set #1 & Data Set #2\n",
      "\n",
      "      Name  Age           City\n",
      "0    John   25       New York\n",
      "1    Jane   30  San Francisco\n",
      "2    Mike   22        Chicago\n",
      "3   Alice   28    Los Angeles\n",
      "4     Bob   35        Seattle\n",
      "0   Emily   28         London\n",
      "1    Jack   24       New York\n",
      "2    Emma   27          Paris\n",
      "3   James   26    Los Angeles\n",
      "4  Olivia   29         Sydney\n",
      "\n",
      "After df.merge_ordered applied to Data Set #1 and Data Set #2\n",
      "...left_on=\"City\", right_on=\"Name\"\n",
      "\n",
      "   NameDataSet1  AgeDataSet1   CityDataSet1 NameDataSet2  AgeDataSet2  \\\n",
      "0         Mike         22.0        Chicago          NaN          NaN   \n",
      "1          NaN          NaN            NaN        Emily         28.0   \n",
      "2          NaN          NaN            NaN         Emma         27.0   \n",
      "3          NaN          NaN            NaN         Jack         24.0   \n",
      "4          NaN          NaN            NaN        James         26.0   \n",
      "5        Alice         28.0    Los Angeles          NaN          NaN   \n",
      "6         John         25.0       New York          NaN          NaN   \n",
      "7          NaN          NaN            NaN       Olivia         29.0   \n",
      "8         Jane         30.0  San Francisco          NaN          NaN   \n",
      "9          Bob         35.0        Seattle          NaN          NaN   \n",
      "\n",
      "  CityDataSet2     Sex  \n",
      "0          NaN     NaN  \n",
      "1       London  Female  \n",
      "2        Paris  Female  \n",
      "3     New York    Male  \n",
      "4  Los Angeles    Male  \n",
      "5          NaN     NaN  \n",
      "6          NaN     NaN  \n",
      "7       Sydney  Female  \n",
      "8          NaN     NaN  \n",
      "9          NaN     NaN  \n",
      "\n",
      "After df.merge_asof applied to Data Set #1 and Data Set #2\n",
      "...left_on=\"Age\"\n",
      "\n",
      "   Name_x  Age         City_x  Name_y    City_y     Sex\n",
      "0   Mike   22        Chicago     NaN       NaN     NaN\n",
      "1   John   25       New York    Jack  New York    Male\n",
      "2  Alice   28    Los Angeles   Emily    London  Female\n",
      "3   Jane   30  San Francisco  Olivia    Sydney  Female\n",
      "4    Bob   35        Seattle  Olivia    Sydney  Female\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Sample DataFrame\n",
    "data1 = {\n",
    "    'Name': ['John', 'Jane', 'Mike', 'Alice', 'Bob'],\n",
    "    'Age': [25, 30, 22, 28, 35],\n",
    "    'City': ['New York', 'San Francisco', 'Chicago', 'Los Angeles', 'Seattle']\n",
    "}\n",
    "\n",
    "data2 = {\n",
    "    'Name': ['Emily', 'Jack', 'Emma', 'James', 'Olivia'],\n",
    "    'Age': [28, 24, 27, 26, 29],\n",
    "    'City': ['London', 'New York', 'Paris', 'Los Angeles', 'Sydney'],\n",
    "    'Sex': ['Female', 'Male', 'Female', 'Male', 'Female']\n",
    "}\n",
    "\n",
    "ExampleDf1 = pd.DataFrame(data1)\n",
    "print(\"Example Data Set #1\\n\",ExampleDf,\"\\n\")\n",
    "ExampleDf2 = pd.DataFrame(data2)\n",
    "print(\"Example Data Set #2\\n\",ExampleDf2,\"\\n\")\n",
    "\n",
    "#DF.QUERY\n",
    "filtered_ExampleDf = ExampleDf.query(\"Age >30 or City == 'Los Angeles'\")\n",
    "print(\"After df.query applied to Data Set #1\\n\\n\",filtered_ExampleDf)\n",
    "\n",
    "#PD.CONCAT\n",
    "concat_Examples = pd.concat([ExampleDf1,ExampleDf2], axis=0,join='inner')\n",
    "print(\"\\nAfter df.concat applied to Data Set #1 & Data Set #2\\n\\n\",concat_Examples)\n",
    "\n",
    "#PD.MERGE_ORDERED\n",
    "ordered_join_Examples = pd.merge_ordered(left=ExampleDf1, right=ExampleDf2, left_on=\"City\", right_on=\"Name\", suffixes=['DataSet1','DataSet2'])\n",
    "print(\"\\nAfter df.merge_ordered applied to Data Set #1 and Data Set #2\\n...left_on=\\\"City\\\", right_on=\\\"Name\\\"\\n\\n\", ordered_join_Examples)\n",
    "\n",
    "#PD.MERGE_ASOF > The merge_asof function finds the nearest match in the right DataFrame for each row in the left DataFrame based on the specified key column. \n",
    "#It returns a merged DataFrame with the matched rows and their corresponding values from the right DataFrame.\n",
    "#IMPORTANT: Must be found in both DataFrames. The data MUST be ordered. Furthermore this must be a numeric column, such as datetimelike, integer, or float.\n",
    "mergeasof_Examples = pd.merge_asof(ExampleDf1.sort_values('Age'),ExampleDf2.sort_values('Age'), on='Age')\n",
    "print(\"\\nAfter df.merge_asof applied to Data Set #1 and Data Set #2\\n...left_on=\\\"Age\\\"\\n\\n\", mergeasof_Examples)\n",
    "\n",
    "#.MELT\n",
    " Examples\n",
    "    --------\n",
    "    >>> df = pd.DataFrame({'A': {0: 'a', 1: 'b', 2: 'c'},\n",
    "    ...                    'B': {0: 1, 1: 3, 2: 5},\n",
    "    ...                    'C': {0: 2, 1: 4, 2: 6}})\n",
    "    >>> df\n",
    "       A  B  C\n",
    "    0  a  1  2\n",
    "    1  b  3  4\n",
    "    2  c  5  6\n",
    "    \n",
    "    >>> pd.melt(df, id_vars=['A'], value_vars=['B'])\n",
    "       A variable  value\n",
    "    0  a        B      1\n",
    "    1  b        B      3\n",
    "    2  c        B      5\n",
    "    \n",
    "    >>> pd.melt(df, id_vars=['A'], value_vars=['B', 'C'])\n",
    "       A variable  value\n",
    "    0  a        B      1\n",
    "    1  b        B      3\n",
    "    2  c        B      5\n",
    "    3  a        C      2\n",
    "    4  b        C      4\n",
    "    5  c        C      6\n",
    "    \n",
    "    The names of 'variable' and 'value' columns can be customized:\n",
    "    \n",
    "    >>> pd.melt(df, id_vars=['A'], value_vars=['B'],\n",
    "    ...         var_name='myVarname', value_name='myValname')\n",
    "       A myVarname  myValname\n",
    "    0  a         B          1\n",
    "    1  b         B          3\n",
    "    2  c         B          5\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
